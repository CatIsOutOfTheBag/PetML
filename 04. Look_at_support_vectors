# работа с методом опорных векторов - одним из видов линейных классификаторов
# функционал направлен на максимальное расширение разделяющей полосы между классами
# если преобразовать его оптимизационную задачу - итоговый классификатор можно представить
# как взвешенную сумму скалярных произведений данного объекта на объекты обучающей выборки
# суть - предсказание на основе сходства нового объекта с объектами обучающей выборки
# поскольку не все коэффициенты оказываются нулевыми, то и сходство определяется не со всеми объектами
# такие объекты называются опорными

# В этой задачке посмотрим как выглядят эти опорные объекты
import pandas as pd
from sklearn.svm import SVC

# загрузим датасет, отдели таргет
data = pd.read_csv("svm-data.csv", delimiter=',', header=None)
X = data.iloc[:, 1:3]
y = data.iloc[:, 0]

# создадим объект класса SVC и обучим классификатор на загруженных данных
clf = SVC(C=100000, random_state=241, kernel='linear')
clf.fit(X, y)

# индексы опорных векторов
print("Вот так выглядят индексы опорных векторов {}".format(clf.support_))
# сами опорные вектора
print("Вот так выглядят сами опорные вектора: \n{}".format(clf.support_vectors_))

# найдем их в выборке
print("\nНайдем их в выборке!")
print(X, '\n')

for i, row in X.iterrows():
    if list(row) in clf.support_vectors_: # если строка совпадает с каким-нибудь из векторов        
        print(list(row), i+1) # выведем ее и ее номер
